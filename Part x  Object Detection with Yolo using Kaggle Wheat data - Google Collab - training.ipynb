{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forked from  [Object Detection with YOLO blog series](https://fairyonice.github.io/tag/object-detection-using-yolov2-on-pascal-voc2012-series.html)\n",
    "\n",
    "Notebooks were only modified as needed, vast majority of the contents are from fairyonice.github repository.\n",
    "\n",
    "My changes covering all notebooks were:\n",
    "- Use Kaggle Wheat Detection data\n",
    "- Migrate to TF 2.x\n",
    "- Modified Data Generator and Loss to remove tensor error\n",
    "- New notebook using albumentations for image & box augmentation\n",
    "- New Kaggle submission notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43788,
     "status": "ok",
     "timestamp": 1599246740005,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "eUu5QbEBREDw",
    "outputId": "b2e3b544-6f6a-4200-fc5a-a3b25353f519"
   },
   "outputs": [],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "#\"\"\"\n",
    "# Google Collab specific stuff....\n",
    "from google.colab import drive\n",
    "#, force_remount=True\n",
    "drive.mount('/content/drive') \n",
    "\n",
    "import os\n",
    "!ls \"/content/drive/My Drive\"\n",
    "\n",
    "USING_COLLAB = True\n",
    "%tensorflow_version 2.x\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13218,
     "status": "ok",
     "timestamp": 1599246760710,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "oUld50SCRED1",
    "outputId": "30cf6558-93a9-49f8-d1eb-a069b521fe60"
   },
   "outputs": [],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "# Upload your \"kaggle.json\" file that you created from your Kaggle Account tab\n",
    "# If you downloaded it, it would be in your \"Downloads\" directory\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13211,
     "status": "ok",
     "timestamp": 1599246760711,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "4qEyMIlyRED4",
    "outputId": "bc993eaf-85f4-4ab6-a51f-2595c67e8da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  kaggle.json  sample_data\n"
     ]
    }
   ],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "# Double check to see what files already exist\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13682,
     "status": "ok",
     "timestamp": 1599246761188,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "Ic5gUSFsRED7",
    "outputId": "cdf5d9d5-6dc8-47bb-88ec-c370ab230501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "# On your VM, create kaggle directory and modify access rights\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22426,
     "status": "ok",
     "timestamp": 1599246769941,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "aQlMt3PrRED9",
    "outputId": "5b67939d-9e4d-4ead-ba52-179549efdd26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling kaggle-1.5.6:\n",
      "  Successfully uninstalled kaggle-1.5.6\n",
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/4a/39400ff9b36e719bdf8f31c99fe1fa7842a42fa77432e584f707a5080063/pip-20.2.2-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 5.0MB/s \n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "Successfully installed pip-20.2.2\n",
      "Collecting kaggle==1.5.6\n",
      "  Downloading kaggle-1.5.6.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 2.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.15.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.8.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.41.1)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.10)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72859 sha256=3674c57e5637ba5fa3d6c6eee4c300d3dcc00f85fbacaa376341acde034b4aa1\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/3e/ff/77407ebac3ef71a79b9166a8382aecf88415a0bcbe3c095a01\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.6\n",
      "Kaggle API 1.5.6\n"
     ]
    }
   ],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "# Install kaggle libs\n",
    "!pip uninstall -y kaggle\n",
    "!pip install --upgrade pip\n",
    "!pip install kaggle==1.5.6\n",
    "!kaggle -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33051,
     "status": "ok",
     "timestamp": 1599246780574,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "8mdONj6xREEA",
    "outputId": "2d5778ea-e538-49bf-e208-ec2c4e1fe70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading global-wheat-detection.zip to /content\n",
      " 98% 593M/607M [00:04<00:00, 102MB/s]\n",
      "100% 607M/607M [00:04<00:00, 131MB/s]\n",
      "drive  global-wheat-detection.zip  kaggle.json\tsample_data\n"
     ]
    }
   ],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "# Download salt files and unzip\n",
    "!kaggle competitions download -c global-wheat-detection\n",
    "!ls\n",
    "!unzip -q global-wheat-detection.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33163,
     "status": "ok",
     "timestamp": 1599246780693,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "kkOVRjkWREED",
    "outputId": "5f874b3a-fc63-45e5-9107-3c03dabdcad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\t\t\t    kaggle.json  sample_submission.csv\ttrain\n",
      "global-wheat-detection.zip  sample_data  test\t\t\ttrain.csv\n"
     ]
    }
   ],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408560,
     "status": "ok",
     "timestamp": 1599247156097,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "bLxoZ8rlRfwP",
    "outputId": "61bcba48-bbc3-433a-dd8d-21e175642e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-04 19:13:00--  https://pjreddie.com/media/files/yolov2.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203934260 (194M) [application/octet-stream]\n",
      "Saving to: ‘yolov2.weights’\n",
      "\n",
      "yolov2.weights      100%[===================>] 194.49M   499KB/s    in 6m 14s  \n",
      "\n",
      "2020-09-04 19:19:15 (532 KB/s) - ‘yolov2.weights’ saved [203934260/203934260]\n",
      "\n",
      "drive\t\t\t    sample_data\t\t   train\n",
      "global-wheat-detection.zip  sample_submission.csv  train.csv\n",
      "kaggle.json\t\t    test\t\t   yolov2.weights\n"
     ]
    }
   ],
   "source": [
    "# Comment out if not using Google Colab\n",
    "\n",
    "!wget https://pjreddie.com/media/files/yolov2.weights\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408663,
     "status": "ok",
     "timestamp": 1599247156206,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "XQjGfxNs4m5p"
   },
   "outputs": [],
   "source": [
    "# if needed, install albumentations\n",
    "\n",
    "#!pip3 install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408655,
     "status": "ok",
     "timestamp": 1599247156207,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "JiLN5DvTREEG",
    "outputId": "dacf9fbe-672f-4d61-c481-5392e0d36aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython']\n",
      "['/content/drive/My Drive/GitHub/MachineLearning/lib', '', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# Setup sys.path to find MachineLearning lib directory\n",
    "\n",
    "try: USING_COLLAB\n",
    "except NameError: USING_COLLAB = False\n",
    "\n",
    "import sys\n",
    "if \"MachineLearning\" in sys.path[0]:\n",
    "    pass\n",
    "else:\n",
    "    print(sys.path)\n",
    "    if USING_COLLAB:\n",
    "        sys.path.insert(0, '/content/drive/My Drive/GitHub/MachineLearning/lib') ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "        print(sys.path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 409067,
     "status": "ok",
     "timestamp": 1599247156627,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "Fqc42vf6qT9A"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "USING_COLLAB = True\n",
    "%tensorflow_version 2.x\n",
    "\n",
    "sys.path.insert(0, '/content/drive/My Drive/GitHub/MachineLearning/lib') ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 409524,
     "status": "ok",
     "timestamp": 1599247157091,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "4wUgvvpq4m5w"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411317,
     "status": "ok",
     "timestamp": 1599247158891,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "LIddFu1fREEN",
    "outputId": "8c40f306-2cdb-406d-d0ee-a6628767ecb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Jul 17 2020, 12:50:27) \n",
      "[GCC 8.4.0]\n",
      "Pandas:  1.0.5\n"
     ]
    }
   ],
   "source": [
    "# imports and display software versions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "print(sys.version)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas: \", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411311,
     "status": "ok",
     "timestamp": 1599247158892,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "so2tQWak4m52",
    "outputId": "27f5f333-d114-4204-f21a-4494a969c061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory at start: : usertime=5.34661 systime=1.262025 mem=349.18359375 mb\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "# if needed....\n",
    "\n",
    "import gc # Garbage Collector\n",
    "\n",
    "import resource # resource usage\n",
    "def memory_usage(point=\"\"):\n",
    "    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "    return '''%s: usertime=%s systime=%s mem=%s mb\n",
    "           '''%(point,usage[0],usage[1],\n",
    "                usage[2]/1024.0 )\n",
    "\n",
    "print(memory_usage(\"Memory at start: \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411300,
     "status": "ok",
     "timestamp": 1599247158892,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "M9lijlsWREEe"
   },
   "outputs": [],
   "source": [
    "# SETUP FILE LOCATIONS BASED ON ENVIRONMENT\n",
    "\n",
    "# For Google Collab\n",
    "MODEL_PATH = \"/content/drive/My Drive/ImageData/KaggleWheat/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "ROOT_PATH=\"./\"\n",
    "\n",
    "# Home\n",
    "#ROOT_PATH = \"/Users/john/Documents/Python-Working/Kaggle-global-wheat-detection/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "#MODEL_PATH = \"/Users/john/Documents/Python-Working/2-Kaggle-Wheat/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "\n",
    "# Kaggle\n",
    "#ROOT_PATH = \"../input/global-wheat-detection/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "#MODEL_PATH = \"../input/wheatmodel/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "\n",
    "##################################################################\n",
    "TRAIN_DATA_PATH = os.path.join(ROOT_PATH, \"train/\")\n",
    "TEST_DATA_PATH = os.path.join(ROOT_PATH, \"test/\")\n",
    "MODEL_NAME = \"model-wheat-yolo-1024-A2-aug2.h5\"\n",
    "YOLO_WEIGHT_PATH = \"./yolov2.weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pokMRxjjREER"
   },
   "source": [
    "## Define anchor box\n",
    "<code>ANCHORS</code> defines the number of anchor boxes and the shape of each anchor box.\n",
    "The choice of the anchor box specialization is already discussed in [Part 1 Object Detection using YOLOv2 on Pascal VOC2012 - anchor box clustering](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html). \n",
    "\n",
    "Based on the K-means analysis in the previous blog post, I will select 4 anchor boxes of following width and height. The width and heights are rescaled in the grid cell scale (Assuming that the number of grid size is 13 by 13.) See [Part 2 Object Detection using YOLOv2 on Pascal VOC2012 - input and output encoding](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html) to learn how I rescal the anchor box shapes into the grid cell scale.\n",
    "\n",
    "Here I choose 4 anchor boxes. With 13 by 13 grids, every frame gets 4 x 13 x 13 = 676 bouding box predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411293,
     "status": "ok",
     "timestamp": 1599247158893,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "S8wdZ1O9REES"
   },
   "outputs": [],
   "source": [
    "\n",
    "ANCHORS = np.array([0.06960639, 0.06130531,\n",
    "                    0.11246752, 0.10739992])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpuBwQ3wREEZ"
   },
   "source": [
    "## Define Label vector containing 1 object classe names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411392,
     "status": "ok",
     "timestamp": 1599247158999,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "G_K7zqK8REEZ"
   },
   "outputs": [],
   "source": [
    "LABELS = ['wheat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxwzbLxqREEd"
   },
   "source": [
    "\n",
    "## Read images and annotations into memory\n",
    "Use the pre-processing code for parsing annotation at [experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2).\n",
    "This <code>parse_annoation</code> function is already used in [Part 1 Object Detection using YOLOv2 on Pascal VOC2012 - anchor box clustering](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html) and saved in my python script. \n",
    "This script can be downloaded at [my Github repository, FairyOnIce/ObjectDetectionYolo/backend](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411525,
     "status": "ok",
     "timestamp": 1599247159139,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "zopv0Rm6REEh",
    "outputId": "b6c24fc5-f149-4ea7-adde-cbc6e07cde0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>bbox</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 222.0, 56.0, 36.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[226.0, 548.0, 130.0, 58.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[377.0, 504.0, 74.0, 160.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 95.0, 109.0, 107.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[26.0, 144.0, 124.0, 117.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height                         bbox   source\n",
       "0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n",
       "1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n",
       "2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n",
       "3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n",
       "4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csvTRAIN_DATA_PATH\n",
    "train_full_df = pd.read_csv(os.path.join(ROOT_PATH, 'train.csv'))\n",
    "train_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 426386,
     "status": "ok",
     "timestamp": 1599247174007,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "SnYYALSw4m6D",
    "outputId": "44f6f468-fbd9-4bdf-bd2b-60825f04b53d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from backend import parse_annotation\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "train_full_list, seen_train_labels = parse_annotation(train_full_df, LABELS, TRAIN_DATA_PATH)\n",
    "\n",
    "# FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.  import pandas.util.testing as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 426382,
     "status": "ok",
     "timestamp": 1599247174009,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "T4oRpldm4m6G",
    "outputId": "bbfd4ad5-ada3-44fa-cd78-3cc8ed031fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train list: 3373, counts: 3373\n"
     ]
    }
   ],
   "source": [
    "train_full_counts = []\n",
    "\n",
    "for value in train_full_list:\n",
    "    cnt = len(value['object'])\n",
    "    if cnt < 20:\n",
    "        group = 1\n",
    "    elif cnt < 30:\n",
    "        group = 2\n",
    "    elif cnt < 40:\n",
    "        group = 3\n",
    "    elif cnt < 50:\n",
    "        group = 4\n",
    "    elif cnt < 60:\n",
    "        group = 5\n",
    "    elif cnt < 70:\n",
    "        group = 6\n",
    "    elif cnt < 100:\n",
    "        group = 7\n",
    "    else:\n",
    "        group = 8\n",
    "\n",
    "    train_full_counts.append(group)\n",
    "\n",
    "print(\"Train list: {}, counts: {}\".format(len(train_full_list), len(train_full_counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 426376,
     "status": "ok",
     "timestamp": 1599247174009,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "t4ITQMfT4m6I",
    "outputId": "dbaddb8a-f38b-440a-f3b7-38605949e303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number:  3373   Train number:  2867   Val number:  506\n"
     ]
    }
   ],
   "source": [
    "#Split into train and val df's stratified by group column\n",
    "\n",
    "train_list, valid_list = train_test_split(train_full_list, test_size=0.15, random_state=42, stratify=train_full_counts)\n",
    "\n",
    "# set lengths and steps\n",
    "train_len = len(train_list)\n",
    "valid_len = len(valid_list)\n",
    "images_list_len = train_len + valid_len\n",
    "\n",
    "print(\"Total number: \", images_list_len, \"  Train number: \", train_len, \"  Val number: \", valid_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yh8uQtBgREEm"
   },
   "source": [
    "## Instantiate batch generator object\n",
    "<code>SimpleBatchGenerator</code> is discussed and used in \n",
    "[Part 2 Object Detection using YOLOv2 on Pascal VOC2012 - input and output encoding](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html).\n",
    "This script can be downloaded at [my Github repository, FairyOnIce/ObjectDetectionYolo/backend](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 427719,
     "status": "ok",
     "timestamp": 1599247175358,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "TEfFRf54REEm",
    "outputId": "8c8bc09f-401d-4784-d017-e68a2024b1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1024, 1024, 3) (8, 32, 32, 2, 6) (8, 1, 1, 1, 130, 4)\n"
     ]
    }
   ],
   "source": [
    "from backend import SimpleBatchGenerator\n",
    "# 1024 -> 32, 416 -> 13,  256 -> 8, 128 -> 4\n",
    "\n",
    "BATCH_SIZE        = 8  # 200\n",
    "IMAGE_H, IMAGE_W  = 1024, 1024 #416, 416\n",
    "GRID_H,  GRID_W   =  32, 32 # 13 , 13\n",
    "TRUE_BOX_BUFFER   = 130 #50\n",
    "BOX               = int(len(ANCHORS)/2)\n",
    "\n",
    "generator_config = {\n",
    "    'IMAGE_H' : IMAGE_H, \n",
    "    'IMAGE_W' : IMAGE_W,\n",
    "    'GRID_H' : GRID_H,  \n",
    "    'GRID_W' : GRID_W,\n",
    "    'LABELS' : LABELS,\n",
    "    'ANCHORS' : ANCHORS,\n",
    "    'BATCH_SIZE' : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : TRUE_BOX_BUFFER,\n",
    "}\n",
    "\n",
    "def normalize(image):\n",
    "    return image / 255.\n",
    "\n",
    "train_batch_generator = SimpleBatchGenerator(train_list, \n",
    "                                             generator_config,\n",
    "                                             norm=normalize, \n",
    "                                             shuffle=True,\n",
    "                                             augment=\"train\")\n",
    "\n",
    "[x_batch, y_batch, b_batch] = train_batch_generator.__getitem__(idx=0)\n",
    "\n",
    "print(x_batch.shape, y_batch.shape, b_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 428332,
     "status": "ok",
     "timestamp": 1599247175977,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "QBbL8Lh94m6N",
    "outputId": "7d81a926-5f4c-4bc9-feae-0eb2fd4cdbf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1024, 1024, 3) (8, 32, 32, 2, 6) (8, 1, 1, 1, 130, 4)\n"
     ]
    }
   ],
   "source": [
    "valid_batch_generator = SimpleBatchGenerator(valid_list, \n",
    "                                             generator_config,\n",
    "                                             norm=normalize, \n",
    "                                             shuffle=True,\n",
    "                                             augment=\"val\")\n",
    "\n",
    "[x_batch_valid, y_batch_valid, b_batch_valid] = valid_batch_generator.__getitem__(idx=0)\n",
    "\n",
    "print(x_batch_valid.shape, y_batch_valid.shape, b_batch_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnSijCDVREEv"
   },
   "source": [
    "## Define model\n",
    "We define a YOLO model.\n",
    "The model defenition function is already discussed in [Part 3 Object Detection using YOLOv2 on Pascal VOC2012 - model](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2012_data_model.html) and all the codes are available at [my Github](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 434225,
     "status": "ok",
     "timestamp": 1599247181876,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "7PTMCSBKREEv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from backend import define_YOLOv2, set_pretrained_weight, initialize_weight\n",
    "CLASS = len(LABELS)\n",
    "\n",
    "(model, y_true, y_pred, true_boxes) = define_YOLOv2(IMAGE_H,IMAGE_W,GRID_H,GRID_W,TRUE_BOX_BUFFER,BOX,CLASS, trainable=True)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "298PkLFpREEy"
   },
   "source": [
    "## Initialize the weights\n",
    "The initialization of weights are already discussed in [Part 3 Object Detection using YOLOv2 on Pascal VOC2012 - model](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2012_data_model.html). \n",
    "All the codes from [Part 3](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2012_data_model.html) are stored at [my Github](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 434987,
     "status": "ok",
     "timestamp": 1599247182643,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "worGKHVwREEy"
   },
   "outputs": [],
   "source": [
    "# Load weights and init layers\n",
    "\n",
    "nb_conv = 22\n",
    "model = set_pretrained_weight(model,nb_conv, YOLO_WEIGHT_PATH)\n",
    "layer = model.layers[-4] # -4 the last convolutional layer\n",
    "\n",
    "initialize_weight(layer,sd=1/(GRID_H*GRID_W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdO03WWiREE0"
   },
   "source": [
    "## Loss function\n",
    "We already discussed the loss function of YOLOv2 implemented by [experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2) in [Part 4 Object Detection using YOLOv2 on Pascal VOC2012 - loss](https://fairyonice.github.io/Part_4_Object_Detection_with_Yolo_using_VOC_2012_data_loss.html).\n",
    "I modified the codes and the codes are available at [my Github](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 434983,
     "status": "ok",
     "timestamp": 1599247182645,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "9PgSEtshREE1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from backend import custom_loss_core \n",
    "\n",
    "#help(custom_loss_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ieF_2ocREE3"
   },
   "source": [
    "Notice that this custom function <code>custom_loss_core</code> depends not only on <code>y_true</code> and <code>y_pred</code> but also the various hayperparameters.\n",
    "Unfortunately, Keras's loss function API does not accept any parameters except <code>y_true</code> and <code>y_pred</code>. Therefore, these hyperparameters need to be defined globaly. \n",
    "To do this, I will define a wrapper function <code>custom_loss</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 435244,
     "status": "ok",
     "timestamp": 1599247182912,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "XhISo-BJREE4",
    "outputId": "a9b8d6c2-b055-4983-c0b9-db3c5cebd6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(290.42746, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "LAMBDA_NO_OBJECT = 1.0\n",
    "LAMBDA_OBJECT = 5.0\n",
    "LAMBDA_COORD = 1.0\n",
    "LAMBDA_CLASS = 1.0\n",
    "    \n",
    "def custom_loss(y_true, y_pred, true_boxes):\n",
    "\n",
    "    return(custom_loss_core(\n",
    "                     y_true,\n",
    "                     y_pred,\n",
    "                     true_boxes,\n",
    "                     GRID_W,\n",
    "                     GRID_H,\n",
    "                     BATCH_SIZE,\n",
    "                     ANCHORS,\n",
    "                     LAMBDA_COORD,\n",
    "                     LAMBDA_CLASS,\n",
    "                     LAMBDA_NO_OBJECT, \n",
    "                     LAMBDA_OBJECT))\n",
    "    \n",
    "# run some values from SimpleGenerator through loss\n",
    "print(custom_loss(y_batch, y_batch, b_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 435816,
     "status": "ok",
     "timestamp": 1599247183492,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "GdmdWItmM0c5"
   },
   "outputs": [],
   "source": [
    "# add loss to model\n",
    "\n",
    "model.add_loss(custom_loss(y_true, y_pred, true_boxes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTLHss4oREE7"
   },
   "source": [
    "## Training starts here! \n",
    "Finally, we start the training here.\n",
    "We only train the final 23rd layer and freeze the other weights.\n",
    "This is because I am unfortunately using CPU environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 435813,
     "status": "ok",
     "timestamp": 1599247183494,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "gXKM_7Y7REE7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "dir_log = \"logs/\"\n",
    "try:\n",
    "    os.makedirs(dir_log)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', \n",
    "                           min_delta=0.001, \n",
    "                           patience=5, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(MODEL_PATH+MODEL_NAME, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False,\n",
    "                             mode='min')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              patience=3, \n",
    "                              verbose=1)\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "#optimizer = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss=None, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 435950,
     "status": "ok",
     "timestamp": 1599247183637,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "y_lfRRIr4m6i"
   },
   "outputs": [],
   "source": [
    "# Load a saved model, because of the Lambda in custom_loss, need to include 'tf' : tf.  Lambda was a workaround to fix known issue.\n",
    "\n",
    "#print(\"loading... \", MODEL_PATH+MODEL_NAME)\n",
    "#model = tf.keras.models.load_model(MODEL_PATH+MODEL_NAME, custom_objects={'tf' : tf, 'custom_loss': custom_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17894198,
     "status": "ok",
     "timestamp": 1599264641890,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "UjtAmEjrREFA",
    "outputId": "8c8ffe1d-e12e-487d-c8cd-f78faf5c65d1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "  2/359 [..............................] - ETA: 2:57 - loss: 6.4149WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3682s vs `on_train_batch_end` time: 0.6269s). Check your callbacks.\n",
      "359/359 [==============================] - ETA: 0s - loss: 1.6376\n",
      "Epoch 00001: val_loss improved from inf to 0.72003, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 561s 2s/step - loss: 1.6376 - val_loss: 0.7200\n",
      "Epoch 2/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.4970\n",
      "Epoch 00002: val_loss improved from 0.72003 to 0.37983, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 556s 2s/step - loss: 0.4970 - val_loss: 0.3798\n",
      "Epoch 3/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.3843\n",
      "Epoch 00003: val_loss improved from 0.37983 to 0.33967, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 571s 2s/step - loss: 0.3843 - val_loss: 0.3397\n",
      "Epoch 4/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.3342\n",
      "Epoch 00004: val_loss did not improve from 0.33967\n",
      "359/359 [==============================] - 584s 2s/step - loss: 0.3342 - val_loss: 0.3492\n",
      "Epoch 5/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.3014\n",
      "Epoch 00005: val_loss improved from 0.33967 to 0.28707, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 596s 2s/step - loss: 0.3014 - val_loss: 0.2871\n",
      "Epoch 6/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.2797\n",
      "Epoch 00006: val_loss did not improve from 0.28707\n",
      "359/359 [==============================] - 585s 2s/step - loss: 0.2797 - val_loss: 0.2899\n",
      "Epoch 7/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.2847\n",
      "Epoch 00007: val_loss improved from 0.28707 to 0.28540, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 586s 2s/step - loss: 0.2847 - val_loss: 0.2854\n",
      "Epoch 8/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.2539\n",
      "Epoch 00008: val_loss improved from 0.28540 to 0.27269, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 585s 2s/step - loss: 0.2539 - val_loss: 0.2727\n",
      "Epoch 9/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.2841\n",
      "Epoch 00009: val_loss improved from 0.27269 to 0.25690, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 592s 2s/step - loss: 0.2841 - val_loss: 0.2569\n",
      "Epoch 10/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.2238\n",
      "Epoch 00010: val_loss did not improve from 0.25690\n",
      "359/359 [==============================] - 581s 2s/step - loss: 0.2238 - val_loss: 0.2794\n",
      "Epoch 11/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.2092\n",
      "Epoch 00011: val_loss improved from 0.25690 to 0.25265, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 563s 2s/step - loss: 0.2092 - val_loss: 0.2527\n",
      "Epoch 12/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.2036\n",
      "Epoch 00012: val_loss did not improve from 0.25265\n",
      "359/359 [==============================] - 574s 2s/step - loss: 0.2036 - val_loss: 0.2633\n",
      "Epoch 13/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1872\n",
      "Epoch 00013: val_loss did not improve from 0.25265\n",
      "359/359 [==============================] - 574s 2s/step - loss: 0.1872 - val_loss: 0.2578\n",
      "Epoch 14/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1798\n",
      "Epoch 00014: val_loss did not improve from 0.25265\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "359/359 [==============================] - 589s 2s/step - loss: 0.1798 - val_loss: 0.2542\n",
      "Epoch 15/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1517\n",
      "Epoch 00015: val_loss improved from 0.25265 to 0.24160, saving model to /content/drive/My Drive/ImageData/KaggleWheat/model-wheat-yolo-1024-A2-aug2.h5\n",
      "359/359 [==============================] - 584s 2s/step - loss: 0.1517 - val_loss: 0.2416\n",
      "Epoch 16/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1445\n",
      "Epoch 00016: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 576s 2s/step - loss: 0.1445 - val_loss: 0.2468\n",
      "Epoch 17/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1408\n",
      "Epoch 00017: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 581s 2s/step - loss: 0.1408 - val_loss: 0.2457\n",
      "Epoch 18/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1388\n",
      "Epoch 00018: val_loss did not improve from 0.24160\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
      "359/359 [==============================] - 588s 2s/step - loss: 0.1388 - val_loss: 0.2433\n",
      "Epoch 19/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00019: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 584s 2s/step - loss: 0.1329 - val_loss: 0.2426\n",
      "Epoch 20/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1328\n",
      "Epoch 00020: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 582s 2s/step - loss: 0.1328 - val_loss: 0.2461\n",
      "Epoch 21/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00021: val_loss did not improve from 0.24160\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-08.\n",
      "359/359 [==============================] - 579s 2s/step - loss: 0.1319 - val_loss: 0.2464\n",
      "Epoch 22/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1322\n",
      "Epoch 00022: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 576s 2s/step - loss: 0.1322 - val_loss: 0.2457\n",
      "Epoch 23/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00023: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 578s 2s/step - loss: 0.1311 - val_loss: 0.2475\n",
      "Epoch 24/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1303\n",
      "Epoch 00024: val_loss did not improve from 0.24160\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 5.000000058430488e-09.\n",
      "359/359 [==============================] - 577s 2s/step - loss: 0.1303 - val_loss: 0.2431\n",
      "Epoch 25/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00025: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 578s 2s/step - loss: 0.1288 - val_loss: 0.2450\n",
      "Epoch 26/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1333\n",
      "Epoch 00026: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 577s 2s/step - loss: 0.1333 - val_loss: 0.2451\n",
      "Epoch 27/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1322\n",
      "Epoch 00027: val_loss did not improve from 0.24160\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.999999969612646e-10.\n",
      "359/359 [==============================] - 579s 2s/step - loss: 0.1322 - val_loss: 0.2472\n",
      "Epoch 28/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00028: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 577s 2s/step - loss: 0.1287 - val_loss: 0.2443\n",
      "Epoch 29/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00029: val_loss did not improve from 0.24160\n",
      "359/359 [==============================] - 575s 2s/step - loss: 0.1309 - val_loss: 0.2456\n",
      "Epoch 30/40\n",
      "359/359 [==============================] - ETA: 0s - loss: 0.1318\n",
      "Epoch 00030: val_loss did not improve from 0.24160\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.999999858590343e-11.\n",
      "359/359 [==============================] - 584s 2s/step - loss: 0.1318 - val_loss: 0.2507\n",
      "Epoch 00030: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "history = model.fit(train_batch_generator, \n",
    "                    steps_per_epoch = len(train_batch_generator), \n",
    "                    epochs = 40, ##50\n",
    "                    validation_data = valid_batch_generator,\n",
    "                    validation_steps = len(valid_batch_generator),\n",
    "                    callbacks = [early_stop, checkpoint, reduce_lr]\n",
    "                    #max_queue_size   = 3 ##\n",
    "                    )\n",
    "\n",
    "# Might get these two warnings:\n",
    "#WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4598s vs `on_train_batch_end` time: 0.7761s). Check your callbacks.\n",
    "#WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x3c4cb72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customizat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17894195,
     "status": "ok",
     "timestamp": 1599264641893,
     "user": {
      "displayName": "John Diekhoff",
      "photoUrl": "",
      "userId": "01476344131643501321"
     },
     "user_tz": 300
    },
    "id": "FdqTs8YB4m6o",
    "outputId": "b03a1acd-af39-4697-d099-a24ed2b54856"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xcdZ3/8ddnbrk36SW9phcq9AK0BS0IKlddBVbBy2ItqOCquCyILqw/8Y6sPnTtrquuIsu6iOxyq4CKguJlkcoK2FB6oQVqqbRN2tK0tGnS3Gc+vz/OSTJpkzRpM5km5/18POZxzpxzZs73dJp5z/f7Ped7zN0REZFoi+W7ACIikn8KAxERURiIiIjCQEREUBiIiAgKAxERQWEgIiIoDET6ZWYvm9lb8l0OkVxTGIiIiMJAZLDMrMDMvmVm28PHt8ysIFw3wcx+YWb7zOxVM/uDmcXCdZ82s1ozazCzF83szfk9EpFuiXwXQGQE+hxwBnAK4MDPgM8DXwBuAGqAynDbMwA3s7nAtcBp7r7dzGYB8eEttkjfVDMQGbzLgZvdfZe71wFfBj4QrmsHpgAz3b3d3f/gwQBgaaAAONHMku7+sru/lJfSi/RCYSAyeFOBLVnPt4TLAJYBm4Bfm9lmM7sRwN03AZ8EbgJ2mdm9ZjYVkWOEwkBk8LYDM7OezwiX4e4N7n6Du88GLgau7+wbcPe73f1N4Wsd+OfhLbZI3xQGIoeXNLPCzgdwD/B5M6s0swnAF4H/ATCzt5vZ8WZmQD1B81DGzOaa2flhR3ML0Axk8nM4IodSGIgc3iMEX96dj0KgGlgLrANWAV8Jtz0B+C3QCDwJ3OLujxH0F3wd2A3sBCYCnxm+QxDpn+nmNiIiopqBiIgoDERERGEgIiIoDEREhBE4HMWECRN81qxZ+S6GiMiI8swzz+x298q+1o+4MJg1axbV1dX5LoaIyIhiZlv6W69mIhERURiIiIjCQEREGIF9BiISTe3t7dTU1NDS0pLvohzTCgsLqaqqIplMDup1CgMRGRFqamooKytj1qxZBOMAysHcnT179lBTU8Nxxx03qNeqmUhERoSWlhbGjx+vIOiHmTF+/Pgjqj0pDERkxFAQHN6R/htFJgxe3NnAvzz6InsPtOW7KCIix5zIhMFfdh/gu49tYnt9c76LIiIjVGlpab6LkDORCYOK4qBnvb6pPc8lERE59uQsDMzsdjPbZWbP9bPNuWa22szWm9njuSoLdIfBvmaFgYgcHXfnU5/6FCeffDILFizgvvvuA2DHjh2cffbZnHLKKZx88sn84Q9/IJ1Oc+WVV3Zt+2//9m95Ln3vcnlq6R3Ad4E7e1tpZhXALcAF7r7VzCbmsCxUFKUAqFcYiIx4X/75ejZs3z+k73ni1DF86R0nDWjbBx98kNWrV7NmzRp2797Naaedxtlnn83dd9/N2972Nj73uc+RTqdpampi9erV1NbW8txzwe/iffv2DWm5h0rOagbuvgJ4tZ9NLgMedPet4fa7clUWyKoZqJlIRI7SE088wdKlS4nH40yaNIlzzjmHlStXctppp/HDH/6Qm266iXXr1lFWVsbs2bPZvHkzH//4x/nVr37FmDFj8l38XuXzorM5QNLMfg+UAd92975qEVcBVwHMmDHjiHZWmIxTkIixr1lnE4mMdAP9BT/czj77bFasWMHDDz/MlVdeyfXXX88HP/hB1qxZw6OPPsqtt97K8uXLuf322/Nd1EPkswM5AbwO+GvgbcAXzGxObxu6+23uvtjdF1dW9jkc92GVFyXVgSwiR+2ss87ivvvuI51OU1dXx4oVKzj99NPZsmULkyZN4qMf/Sgf+chHWLVqFbt37yaTyfCe97yHr3zlK6xatSrfxe9VPmsGNcAedz8AHDCzFcAiYGOudlhRnFQzkYgctXe96108+eSTLFq0CDPjG9/4BpMnT+ZHP/oRy5YtI5lMUlpayp133kltbS0f+tCHyGQyAHzta1/Lc+l7l88w+BnwXTNLACng9UBOu9krilJqJhKRI9bY2AgEV/kuW7aMZcuW9Vh/xRVXcMUVVxzyumO1NpAtZ2FgZvcA5wITzKwG+BKQBHD3W939eTP7FbAWyAA/cPc+T0MdCuXFSba92pTLXYiIjEg5CwN3XzqAbZYByw633VCpKErynE4tFRE5RGSuQAb1GYiI9CViYZCiuT1NS3s630URETmmRCoMyouCC8/2q6lIRKSHSIWBxicSEeldtMIgHJ9I/QYiIj1FKwy6xifStQYiklv93fvg5Zdf5uSTTx7G0hxepMKgs89AzUQiIj3l8wrkYacb3IiMEr+8EXauG9r3nLwALvx6n6tvvPFGpk+fzjXXXAPATTfdRCKR4LHHHmPv3r20t7fzla98hUsuuWRQu21paeHqq6+murqaRCLBN7/5Tc477zzWr1/Phz70Idra2shkMjzwwANMnTqV9773vdTU1JBOp/nCF77AkiVLjuqwO0UqDEoLEsRjpiEpRGTQlixZwic/+cmuMFi+fDmPPvoo1113HWPGjGH37t2cccYZXHzxxYO6Kf33vvc9zIx169bxwgsv8Na3vpWNGzdy66238olPfILLL7+ctrY20uk0jzzyCFOnTuXhhx8GoL6+fsiOL1JhYGZUFOnCM5ERr59f8Lly6qmnsmvXLrZv305dXR1jx45l8uTJ/MM//AMrVqwgFotRW1vLK6+8wuTJkwf8vk888QQf//jHAZg3bx4zZ85k48aNnHnmmXz1q1+lpqaGd7/73ZxwwgksWLCAG264gU9/+tO8/e1v56yzzhqy44tUnwEE4xOpz0BEjsSll17K/fffz3333ceSJUu46667qKur45lnnmH16tVMmjSJlpaWIdnXZZddxkMPPURRUREXXXQR//u//8ucOXNYtWoVCxYs4POf/zw333zzkOwLIlYzgGB8IvUZiMiRWLJkCR/96EfZvXs3jz/+OMuXL2fixIkkk0kee+wxtmzZMuj3POuss7jrrrs4//zz2bhxI1u3bmXu3Lls3ryZ2bNnc91117F161bWrl3LvHnzGDduHO9///upqKjgBz/4wZAdW/TCoDjFroahSW4RiZaTTjqJhoYGpk2bxpQpU7j88st5xzvewYIFC1i8eDHz5s0b9Hv+/d//PVdffTULFiwgkUhwxx13UFBQwPLly/nv//5vkskkkydP5rOf/SwrV67kU5/6FLFYjGQyyfe///0hOzZz9yF7s+GwePFir66uPuLXX3/fav708qs88enzh7BUIpJrzz//PPPnz893MUaE3v6tzOwZd1/c12si2WegZiIRkZ6i10xUlKKhtYP2dIZkPHJZKCLDaN26dXzgAx/osaygoICnn346TyXqW/TCoLh75NLxpQV5Lo2IDIa7D+oc/nxbsGABq1evHtZ9HmnTf85+GpvZ7Wa2y8z6vZWlmZ1mZh1m9je5Kks2jVwqMjIVFhayZ8+eI/6yiwJ3Z8+ePRQWFg76tbmsGdwBfBe4s68NzCwO/DPw6xyWo4eu8YnUbyAyolRVVVFTU0NdXV2+i3JMKywspKqqatCvy+U9kFeY2azDbPZx4AHgtFyV42AVxcEw1vUakkJkREkmkxx33HH5LsaolbceVDObBrwLOOyJsmZ2lZlVm1n10f4qqFDNQETkEPk8neZbwKfdPXO4Dd39Nndf7O6LKysrj2qn3fc0UBiIiHTK59lEi4F7wzMDJgAXmVmHu/80lzstK0xipg5kEZFseQsDd+9q/DOzO4Bf5DoIAOIxY0xhknrd7UxEpEvOwsDM7gHOBSaYWQ3wJSAJ4O635mq/A1GhkUtFRHrI5dlESwex7ZW5KkdvdE8DEZGeIjkeQ3lxSjUDEZEskQyD4J4G6jMQEekUzTBQn4GISA/RDIOiJPXN7WQyGuNERAQiGgblxSncoaGlI99FERE5JkQyDLqGpND4RCIiQFTDQENSiIj0EO0wUCeyiAgQ0TAoLwqGsd6n00tFRICIhkFnzaBeNQMRESCiYaC7nYmI9BTJMEjGY5QWJBQGIiKhSIYBBLUDnVoqIhKIbBhUFCepV81ARASIeBjo1FIRkUB0w6AopVNLRURCkQ2DMeFgdSIiksMwMLPbzWyXmT3Xx/rLzWytma0zsz+a2aJclaU3FcXB3c7cNXKpiEguawZ3ABf0s/4vwDnuvgD4J+C2HJblEBVFSToyzoG29HDuVkTkmJSzMHD3FcCr/az/o7vvDZ8+BVTlqiy96R6sTv0GIiLHSp/Bh4FfDucOu8cnUr+BiEgi3wUws/MIwuBN/WxzFXAVwIwZM4ZkvxqfSESkW15rBma2EPgBcIm77+lrO3e/zd0Xu/viysrKIdm37mkgItItb2FgZjOAB4EPuPvG4d5/RWczkYakEBHJXTORmd0DnAtMMLMa4EtAEsDdbwW+CIwHbjEzgA53X5yr8hxMNQMRkW45CwN3X3qY9R8BPpKr/R9OYTJOQSKmPgMREY6ds4nyIrjwTM1EIiLRDoOilJqJRESIeBiUF2t8IhERiHgYVGiwOhERIOphEA5WJyISdREPg5SuMxARIeJhUF6UpKU9Q0u7Ri4VkWiLdBhofCIRkUC0w0Ajl4qIAFEPA93TQEQEiHgYlBeFYaBmIhGJuEiHQVefgZqJRCTiIh4GGsZaRAQiHgYlqTiJmKkDWUQiL9JhYGbBVcjqMxCRiIt0GEDQiaw+AxGJusiHgYakEBFRGFBRpMHqRERyFgZmdruZ7TKz5/pYb2b2HTPbZGZrzey1uSpLf8o1cqmISE5rBncAF/Sz/kLghPBxFfD9HJalTxVFKY1NJCKRl7MwcPcVwKv9bHIJcKcHngIqzGxKrsrTl4riJI2tHbSnM8O9axGRY0Y++wymAduynteEyw5hZleZWbWZVdfV1Q1pITRyqYjICOlAdvfb3H2xuy+urKwc0vfuGp9I/QYiEmH5DINaYHrW86pw2bDqHJKiXqeXikiE5TMMHgI+GJ5VdAZQ7+47hrsQFaoZiIiQyNUbm9k9wLnABDOrAb4EJAHc/VbgEeAiYBPQBHwoV2XpT/c9DRQGIhJdOQsDd196mPUOXJOr/Q9U193O1IEsIhE2IjqQc6msMIEZ1OtuZyISYQMKAzMrMbNYOD/HzC42s2RuizY8YjGjvEgjl4pItA20ZrACKDSzacCvgQ8QXGE8Kmh8IhGJuoGGgbl7E/Bu4BZ3vxQ4KXfFGl7lxSnVDEQk0gYcBmZ2JnA58HC4LJ6bIg2/iqKk+gxEJNIGGgafBD4D/MTd15vZbOCx3BVreOluZyISdQM6tdTdHwceBwg7kne7+3W5LNhwUp+BiETdQM8mutvMxphZCfAcsMHMPpXbog2f8uIU+1vaSWc830UREcmLgTYTneju+4F3Ar8EjiM4o2hUqChK4g4NLaodiEg0DTQMkuF1Be8EHnL3dmDU/IzWyKUiEnUDDYP/AF4GSoAVZjYT2J+rQg23rvGJ1IksIhE10A7k7wDfyVq0xczOy02Rhl/3YHU6vVREommgHcjlZvbNzruNmdm/EtQSRoXyos57GqhmICLRNNBmotuBBuC94WM/8MNcFWq4aRhrEYm6gQ5h/Rp3f0/W8y+b2epcFCgf1IEsIlE30JpBs5m9qfOJmb0RaM5NkYZfMh6jtCDBPt36UkQiaqA1g78D7jSz8vD5XuCK3BQpP8qLktSrZiAiETWgmoG7r3H3RcBCYKG7nwqcf7jXmdkFZvaimW0ysxt7WT/DzB4zs2fNbK2ZXTToIxgiGp9IRKJsUHc6c/f94ZXIANf3t62ZxYHvARcCJwJLzezEgzb7PLA8DJf3AbcMpjxDqaI4qVNLRSSyjua2l3aY9acDm9x9s7u3AfcClxy0jQNjwvlyYPtRlOeoVBTpngYiEl1HEwaHG45iGrAt63lNuCzbTcD7zawGeAT4eG9vZGZXdV7jUFdXd4TF7V95sfoMRCS6+g0DM2sws/29PBqAqUOw/6XAHe5eBVwE/HfnvZazuftt7r7Y3RdXVlYOwW4PVRHeB9l91Ay5JCIyYP2eTeTuZUfx3rXA9KznVeGybB8GLgj39aSZFQITgF1Hsd8jUlGcJJ1xGls7KCtMDvfuRUTy6miaiQ5nJXCCmR1nZimCDuKHDtpmK/BmADObDxQCuWkHOoyKcEgKXXgmIlGUszBw9w7gWuBR4HmCs4bWm9nNZnZxuNkNwEfNbA1wD3Cl56mdpjwckkLjE4lIFA30orMj4u6PEHQMZy/7Ytb8BuCNuSzDQFVoSAoRibBcNhONKBXFYTORhqQQkQhSGIQ0cqmIRJnCINQ5cqn6DEQkiqITBgd2w7r7Id3R6+rCZJzCZExDUohIJEUnDDb/Hh74MLyyrs9NKopSaiYSkUiKThjMODOYbn2qz000cqmIRFV0wqB8GlTMgC1/7HsT3dNARCIqOmEAQe1g61PQx3VtQc1AfQYiEj0RC4Mz4MAueHVzr6vVZyAiURWxMHhDMO2j30B9BiISVdEKgwlzoGgsbO2936C8OElbR4aW9vQwF0xEJL+iFQaxGEw/o++agUYuFZGIilYYQNBvsGcTNB46UnbXkBTqRBaRiIleGMzs7Dd48pBVGrlURKIqemEwZREkCnttKirXYHUiElHRC4NEAUx7Xe81g3AY63o1E4lIxEQvDCC4+GzHGmg70GOxmolEJKpyGgZmdoGZvWhmm8zsxj62ea+ZbTCz9WZ2dy7L02XGmeBpqFnZY3FxKk4ybrrWQEQiJ2dhYGZx4HvAhcCJwFIzO/GgbU4APgO80d1PAj6Zq/L0MP00wA7pNzAzynUVsohEUC5rBqcDm9x9s7u3AfcClxy0zUeB77n7XgB335XD8nQrLIdJJ/fRb5BUn4GIRE4uw2AasC3reU24LNscYI6Z/Z+ZPWVmF/T2RmZ2lZlVm1l1Xd2h1wcckZlnwraVh9zspqIoqZqBiEROvjuQE8AJwLnAUuA/zazi4I3c/TZ3X+zuiysrK4dmzzPOgPYDsHNtj8UVxQoDEYmeXIZBLTA963lVuCxbDfCQu7e7+1+AjQThkHt93OymvCil+yCLSOTkMgxWAieY2XFmlgLeBzx00DY/JagVYGYTCJqNeh9feqiNmQoVMw/pNwhqBuozEJFoyVkYuHsHcC3wKPA8sNzd15vZzWZ2cbjZo8AeM9sAPAZ8yt335KpMh5hxZhAGWTe7qShKcqAtTVtHZtiKISKSb4lcvrm7PwI8ctCyL2bNO3B9+Bh+M86AtfcGN7sZ/xqge7C6+uZ2KssK8lIsEZHhlu8O5Pzq6jfobioq15AUIhJB0Q6DyrlQNA62dIeBhqQQkSiKdhiYBU1FWTWDCo1cKiIRFO0wgCAMXn0JGoOLn7vudqbTS0UkQhQGMzpvdhNcb1De1UykPgMRiQ6FQdfNboKmorLCBGbowjMRiRSFQSIF0xZ3hUEsZpRrfCIRiRiFAQSD1u1YC62NAIwrSfH8jv141sVoIiKjmcIAgk5kT0NtNQBXnDmL6i17ufPJLXkumIjI8FAYAFSdDhbrut7gg2fO5Ly5lXz1kefZ+EpDngsnIpJ7CgOAwjEw6aSufgMz4xt/s4iyggTX3fMsLe3pPBdQRCS3FAadZrwBaqohHXQcV5YVsOzShbyws4Flj76Y58KJiOSWwqBTLze7OX/eJD545kz+64m/sGLjEN1hTUTkGKQw6NTHzW4+e9F8jp9Yyg0/XsOrB3QhmoiMTgqDTmOmwNhZh9zspjAZ59vvO4X6pnY+/cBanW4qIqOSwiDbjDODmsFBX/gnTS3n/10wl99seIW7/7Q1T4UTEckdhUG2GWfAgTrY89Ihq/72jcdx1gkT+KdfbGDTrsY8FE5EJHdyGgZmdoGZvWhmm8zsxn62e4+ZuZktzmV5DquXm910isWMf7l0EUXJOJ+491ndFlNERpWchYGZxYHvARcCJwJLzezEXrYrAz4BPJ2rsgzYhDnBzW4O6kTuNGlMIV9/z0LWb9/Pv/5Gp5uKyOiRy5rB6cAmd9/s7m3AvcAlvWz3T8A/Ay05LMvAmIX9Bn/sc5O3nTSZpafP4LYVm/njpt3DWDgRkdzJZRhMA7ZlPa8Jl3Uxs9cC09394f7eyMyuMrNqM6uuq8vx+f4zzoBXN0PDK31u8oW3z+e48SVcv3yN7nsgIqNC3jqQzSwGfBO44XDbuvtt7r7Y3RdXVlbmtmAzw5vdbOu9qQigOJXg2+87ld2NrXzmwXVkMjrdVERGtlyGQS0wPet5VbisUxlwMvB7M3sZOAN4KO+dyJMXQqIInr0LDuzpc7MFVeX849vm8svndnLpfzzJizs1oJ2IjFy5DIOVwAlmdpyZpYD3AQ91rnT3enef4O6z3H0W8BRwsbtX57BMh5dIwRuuhT//Gr61AH57U5+h8LGzZ/Mvly5ic10jf/2dP/DPv3qB5jYNaiciI0/OwsDdO4BrgUeB54Hl7r7ezG42s4tztd8hcf7n4ZqnYe6F8MS34NsL4bdfhqZXe2xmZvzN66r43Q3n8q5Tp/H937/E2761gsc1jpGIjDA20oZXWLx4sVdXD2PlYdcLsOIb8NyDkCqB138MzrwWiscdsumTL+3hcz9dx+a6A1y8aCqff/t8JpYVDl9ZRUT6YGbPuHufzfAKg4Ha9Tw8/g1Y/5N+Q6G1vYMf/eYZfvXHlcxK7OHyeXFeW96A7a+FWWfB6VcFp7CKiAwjhcFQ6xEKpbBoCXS0Qn0N1G8Lph09L5lotiISJeNJNtbAvLfDO2+BwvI8HYCIRJHCIFde2RA0H214CEomQHkVlE8PphUzoHw6Xj6Nn7+c4Eu/rqGhtYNvz3qKi3beglXMhCX/A5MOuSBbRCQnFAa55n7YZp9XD7Sx7NEXeGBVLQvTG7it6LuMsWb87d8ieer7hqmgIhJlhwsDjVp6tAbQ/j+uJMXX3r2Qpz7zZt564Tv5cMG/Ut0+i+TPPsYzt/wtW3ftHYaCioj0TTWDPMhknCc27qTll1/krfXLeTZzPHdWfZmL3nQa58+bSDymDmYRGVpqJjrG7a2+n+JfXkdTOs61bdfwl7LTWHLaDN48fyInThlDTMEgIkNAYTAS7P4zft/7oe5F7i+/gv/3yltwYowrSfHG4ydw1gnBY0p5Ub5LKiIjlMJgpGg7AD//BKz7MW1Vb2CHVfLqvv00NDZgHS0UWhvliTRjU2lK4x0UeiuWboVYAgrHQEEZFJRDQRleUEY6VUZrrJiWWAkHrISOgnJmnLCAxMS5wXUSR6kjnaG+uZ3xpQVDcPAikmuHC4PEcBZG+pEqgXf/J1SdTur/vs1M28bMVCE+qZBmL2BvWzF1zcbmZjiQSdJOAWVlZZQlnXhrI4kDByhI76MwXUuRN1FKE2U0UWJpxnfu4/Fg0lE2LQiFCXNhwglQOTe4sU9J5WE7xDftauDHz9Twk1W17Gpo5WNnz+Yf3zaXZLyXcxEyaahdBZt/H1ycN/8dUDpxKP/VRGSIqGYwwrS0p1m1ZS8r/ryb/9u0mwOtHYwpSgaPwkQ4TTKmKMGYwiQVqQxj4y207d/F+rXVtOx4gddYLQsLdzEzU0Mi3dz95oUVQShMOgmmLIKpp8DEE6lvj/GLtdv5cXUNq7ftIx4zzps7kfKiJA+squF1M8fy70tPZWpFEezfAS/9Djb9Fl56DFr2db+/xWDmG+Gkd8H8i6E0x8ORi0gXNRNJDzvqm3lwVS3Lq7exdU8jxxfs57LZzbx14n6mdmzFdm+Enc9Baz0AHSR40atYl55FXel8Zpx8Jm94wzlUjqsA4OfPvsyDP7mfs2NrubTiRUr3vRDsqHQSHP8WOP7NMPs8aNgRXLW9/qew589HFgxtTbBvK+x9OXgAFFVA0djgUdg5XwHx5ND/44mMYAoD6VUm4/zp5VdZvnIbjzy3g5b2DPMml3Hp4unsbWzlqVXPMLHxBV6X2so5ZbXMavszidbwV77FoXIelE2CrU9D+wHaSbAyPYfWWedz1oVLSExZ0HuTkzvs2tB3MMw+Fxp3dX/hZz8adw78AFOlWQFRETTDpUogWRysS5VAKmu+c/mYqTD+NZBUZ72MLgoDOaz9Le38fM12lq/cxpqaemIGZ8+p5NLXTefN8ydSmIwHX+L122DHmuCxfTXs3x7cJvT4t9BS9Qa+/Ott3POnrZw2ayzfWXrq4c9+codX1sOGnwbhsGfTQRsYjJkGY2cFj3GzYOxxwXzFTIjFoXkvNO+DlnDanD3dGzRTNe+D9gNBJ33bgaCG0dYI3te9JywYUmTCnPBxQjCtnAvF4/vvV+loDctTH+y7pT7oO4nFg9CLJYL5WCII1Visez6ehHgKEoWQKOiexuID/SgHxh3SbcG/RXtT8O/RfgDSHZ0bBNscMg3XQXAsnQ8snLeeyy0GmQ7ItEO6PdhnOnu+LVifbgvev6AUCjpPhuh8hM8H+m+QyfR8787j7T74nv8OncsyHcFn1/najnCabg3K29EazLsHn0myKLgJVrIw+CGRKAyWdS6PJ4N/Dw/fu/ORbg/+P2Qv62gN/79k/Z9p3tc9n73slMvgjKuP4ENXGMggba5rpKQgwaQxRzb09s9W1/KZB9dRmIzzzfcu4ty5A+ww7gyG2me6A6BievCHlwvuwR9hexgMnUGxbyvs3pj12AQdWf0qRWODYBgzLdj+4D/egwYpHBKxRC8BkTjoizf7izie9UXt3V/2bU3h8R7oJwiPUcmSMBxKAcsKlrae4XKsHJeFJ1R45sjfI1kcDGhZWBFMiyqC2vOiIxvCRmEgw+6lukauuWsVL+xs4JrzXsM/vGUOid7ONhoJMhnYXwN12QHxZ9hfG3w5FVX0/GPtmh8b/iGXB1/cngl/CYa/Cj0dzqfD+Y7uL7WOluCXaUdLEFid03Q4394SvMYz4cO75zPZy8Mvoq5msOLgSzVVHD4v6Tnt6mcxsM6p9Zx28YP27z3365mgjLFEUNvpnMZTEM+a71wOQUi1NkDr/nCa/djfPcXC1yez3rOX+VgiqxaXVfbeanY9Xp8KArfHfBLiBcGXfEdL+Dk0BZ9FRzO0h4/s5Z3v21kT7OsRT2b936no/n+TSJP5QDcAAAtnSURBVA3N/+Guw1YYSB40t6X58s/Xc+/KbZw+axyf++v5zJ1cFjQ5iciwy2sYmNkFwLeBOPADd//6QeuvBz4CdAB1wN+6+5b+3lNhMLL85NkaPvvgczS3p4nHjOMmlDB/yhjmTylj/uQxzJ8yhkljCjDd8Eckp/IWBmYWBzYCfwXUACuBpe6+IWub84Cn3b3JzK4GznX3Jf29r8Jg5Nm1v4XqLXt5fsf+8NFA7b7udvixxUnmhcEwZ1Ip40pSVBSnqChOUlGUpLw4SUFCNQqRo5HPK5BPBza5++awIPcClwBdYeDuj2Vt/xTw/hyWR/Jk4phCLlowhYsWTOlaVt/czgs79vPCzoaukLj7T1toae+9w60oGaeiOEl5UTIMiRSzJpSwsKqcBdPKqRpbpNqFyFHIZRhMA7ZlPa8BXt/P9h8GfpnD8sgxpLwoyetnj+f1s7sGyyCdcbbva6a+uZ19Te3sa25jX1N7+LwtXNZOfVM7m+oa+d0Lr9CeDmq240pSLJhWzsKqchZWVbCwqvyIz4gSiaJjYmwiM3s/sBg4p4/1VwFXAcyYMWMYSybDKR4zpo8rZvoAt2/tSPPizgbW1tSztmYfa2vqueX3u0lngoCYWFbAwqoKTpwa9EtUlhZQWVbAhHCqzmyRbrkMg1ro8XddFS7rwczeAnwOOMfdW3t7I3e/DbgNgj6DoS+qjEQFiXhYC6gAZgLBWUwbdtSzZls962qDkPjdC6/QW9dYWUGiRzhMKE1x6oyx/NWJkygpOCZ+J4kMm1x2ICcIOpDfTBACK4HL3H191janAvcDF7j7nwfyvupAlsFq68iw50ArdQ2t7G7snLZR19BKXefzhlZ2NbTS2NpBYTLGX504mUsWTeXsOZWkEiP0GgmRLHnrQHb3DjO7FniU4NTS2919vZndDFS7+0PAMqAU+HHY+bfV3S/OVZkkmlKJGFPKiw47PEYm41Rv2cvPVtfy8Lod/HzNdsqLkly0YAqXnDKV02eN053nZNTSRWcivWjryPDEpjp+tno7v9nwCk1taaaUF3LxoqlcfMpUTpwyRmcvyYiiK5BFjlJTWwe/fX4XP3u2lsc31tGRcaaPK2JaRRETSnv2OXQ+nxA+1/URcqzQnc5EjlJxKhHUCBZNZe+BNn753E6e2FRHXUMr67fvZ3dDKw2tHb2+tqwwuMlQSUGcolSCklSc4lSc4lQiWJYMp6k4xck4qUScZNxIJWKk4jFSiRjJcJq9rLQgwdjiFEUphY0MDYWByCCMLUlx2etncNnre57i3NKeZndj0DG9O+yo7ny+v6Wd5rY0B9rSNLV2sK+pnaa2Dpra0jS1pTnQ1tHr2U4DUZCIMTa8WntscYqxJUkqilOMDZ+XFSZ6NGcd3LCVvS5mwem9MbNwSvd8zIiH82YE49QRjk+Hh1Nw965lne/f+boej16WxcJtY7Gs/Zp1lyt8nogFj6AsA2uqa2lP09jaQUNLB40tHTS0tnfNN7Z2kHHvt2yJcN/JRIyiZHegF4fhXpSKk4rHDlsed6cj47R1ZIJHOkN7OkM642SccOqkM95jPpjClPJCpo8rHtAxD5bCQGQIFCbjVI0tpmrs4P9Q3Z3WjgxNbWna091fEm0dmV6eB18kDS3t7G0KLsbb29TG3qZ29h5o48WdDV0X53VebzGaJePBl3UyFiMRNxLxGMmYEY8HX8oHWtM0tnTQlj6KoaQHKB4zipNxiguCoMh4zy/9zunRtMz/3Tmv4cYL5w1dobMoDETyzMwoTMaH9CI4d2d/SwcNLe1Zy/rbnuDXpzuZ3n6lhss7f8FaOMq1mfU9Dz1+5aaz5zOHLu/89Zvp3JcH+8pk/UrOhL+sO9JORzoTzGec9nQmWJbpXu7ulBYmKCtMUlqQoKwweJQWJMNpomsajxkdmWC/Hb2UrfPRls7QHNbomto6ump8zVk1veawthcz69G8V3BQU19XE2A8FtZ8umtEQQ2pu5bSOV81Nnd34FMYiIxCZkZ5UTCWk8hA6GoaERFRGIiIiMJARERQGIiICAoDERFBYSAiIigMREQEhYGIiDACRy01szpgyxG+fAKwewiLcywYbcc02o4HRt8xjbbjgdF3TL0dz0x3r+zrBSMuDI6GmVX3N4TrSDTajmm0HQ+MvmMabccDo++YjuR41EwkIiIKAxERiV4Y3JbvAuTAaDum0XY8MPqOabQdD4y+Yxr08USqz0BERHoXtZqBiIj0QmEgIiLRCQMzu8DMXjSzTWZ2Y77LMxTM7GUzW2dmq82sOt/lGSwzu93MdpnZc1nLxpnZb8zsz+F0bD7LOFh9HNNNZlYbfk6rzeyifJZxMMxsupk9ZmYbzGy9mX0iXD4iP6d+jmckf0aFZvYnM1sTHtOXw+XHmdnT4XfefWaW6vd9otBnYGZxYCPwV0ANsBJY6u4b8lqwo2RmLwOL3X1EXixjZmcDjcCd7n5yuOwbwKvu/vUwtMe6+6fzWc7B6OOYbgIa3f1f8lm2I2FmU4Ap7r7KzMqAZ4B3AlcyAj+nfo7nvYzcz8iAEndvNLMk8ATwCeB64EF3v9fMbgXWuPv3+3qfqNQMTgc2uftmd28D7gUuyXOZIs/dVwCvHrT4EuBH4fyPCP5QR4w+jmnEcvcd7r4qnG8AngemMUI/p36OZ8TyQGP4NBk+HDgfuD9cftjPKCphMA3YlvW8hhH+HyDkwK/N7BkzuyrfhRkik9x9Rzi/E5iUz8IMoWvNbG3YjDQimlQOZmazgFOBpxkFn9NBxwMj+DMys7iZrQZ2Ab8BXgL2uXtHuMlhv/OiEgaj1Zvc/bXAhcA1YRPFqOFBG+ZoaMf8PvAa4BRgB/Cv+S3O4JlZKfAA8El335+9biR+Tr0cz4j+jNw97e6nAFUELSHzBvseUQmDWmB61vOqcNmI5u614XQX8BOC/wQj3Sthu25n++6uPJfnqLn7K+Efawb4T0bY5xS2Qz8A3OXuD4aLR+zn1NvxjPTPqJO77wMeA84EKswsEa467HdeVMJgJXBC2LueAt4HPJTnMh0VMysJO8AwsxLgrcBz/b9qRHgIuCKcvwL4WR7LMiQ6vzRD72IEfU5h5+R/Ac+7+zezVo3Iz6mv4xnhn1GlmVWE80UEJ8o8TxAKfxNudtjPKBJnEwGEp4p9C4gDt7v7V/NcpKNiZrMJagMACeDukXZMZnYPcC7BcLuvAF8CfgosB2YQDFX+XncfMR2yfRzTuQTNDw68DHwsq739mGZmbwL+AKwDMuHizxK0s4+4z6mf41nKyP2MFhJ0EMcJfuAvd/ebw++Ie4FxwLPA+929tc/3iUoYiIhI36LSTCQiIv1QGIiIiMJAREQUBiIigsJAREQITkkUkYOYWZrg9MNO97r71/NVHpFc06mlIr0ws0Z3L813OUSGi5qJRAYhvIfEN8L7SPzJzI4Pl88ys/8NBzr7nZnNCJdPMrOfhGPNrzGzN4RXjz8cPn/OzJbk96hEFAYifSnKutHJ6oO+sOvdfQHwXYKr2gH+HfiRuy8E7gK+Ey7/DvC4uy8CXgusBy4Atrv7ovCeB78ajgMS6Y+aiUR60VczUXhDofPdfXM44NlOdx9vZrsJbprSHi7f4e4TzKwOqMoeBsDM5gC/Bu4DfuHufxiWgxLph2oGIoPnfcwP7MXuGwlqCeuAr5jZF4eqYCJHSmEgMnhLsqZPhvN/JBgNF+BygsHQAH4HXA1dNyApN7OpQJO7/w+wjCAYRPJKzUQivejl1NJfufuNYTPRfQQ3FGoluJf2JjObCfyQYLTSOuBD7r7VzCYBtwGzgTRBMIwhCIEM0A5c7e7Vw3NkIr1TGIgMQhgGi919d77LIjKU1EwkIiKqGYiIiGoGIiKCwkBERFAYiIgICgMREUFhICIiwP8Hff44FxzqNLMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graph training...\n",
    "history_df = pd.DataFrame(history.history)\n",
    "plt.figure()\n",
    "history_df[['loss', 'val_loss']].plot(title=\"Loss\")\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#history_df\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 5 Object Detection using Google Collab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
