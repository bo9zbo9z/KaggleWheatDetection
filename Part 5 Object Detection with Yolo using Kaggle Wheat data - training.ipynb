{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forked from  [Object Detection with YOLO blog series](https://fairyonice.github.io/tag/object-detection-using-yolov2-on-pascal-voc2012-series.html)\n",
    "\n",
    "Notebooks were only modified as needed, vast majority of the contents are from fairyonice.github repository.\n",
    "\n",
    "My changes covering all notebooks were:\n",
    "- Use Kaggle Wheat Detection data\n",
    "- Migrate to TF 2.x\n",
    "- Modified Data Generator and Loss to remove tensor error\n",
    "- New notebook using albumentations for image & box augmentation\n",
    "- New Kaggle submission notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is the fifth blog post of [Object Detection with YOLO blog series](https://fairyonice.github.io/tag/object-detection-using-yolov2-on-pascal-voc2012-series.html). This blog finally train the model using the scripts that are developed in the [previous blog posts](https://fairyonice.github.io/tag/object-detection-using-yolov2-on-pascal-voc2012-series.html). \n",
    "I will use PASCAL VOC2012 data. \n",
    "This blog assumes that the readers have read the previous blog posts - [Part 1](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html), [Part 2](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html), [Part 3](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2012_data_model.html), [Part 4](https://fairyonice.github.io/Part_4_Object_Detection_with_Yolo_using_VOC_2012_data_loss.html).\n",
    "\n",
    "## Andrew Ng's YOLO lecture\n",
    "- [Neural Networks - Bounding Box Predictions](https://www.youtube.com/watch?v=gKreZOUi-O0&t=0s&index=7&list=PL_IHmaMAvkVxdDOBRg2CbcJBq9SY7ZUvs)\n",
    "- [C4W3L06 Intersection Over Union](https://www.youtube.com/watch?v=ANIzQ5G-XPE&t=7s)\n",
    "- [C4W3L07 Nonmax Suppression](https://www.youtube.com/watch?v=VAo84c1hQX8&t=192s)\n",
    "- [C4W3L08 Anchor Boxes](https://www.youtube.com/watch?v=RTlwl2bv0Tg&t=28s)\n",
    "- [C4W3L09 YOLO Algorithm](https://www.youtube.com/watch?v=9s_FpMpdYW8&t=34s)\n",
    "\n",
    "## Reference\n",
    "- [You Only Look Once:Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640.pdf) \n",
    "\n",
    "- [YOLO9000:Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf)\n",
    " \n",
    "- [experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2)\n",
    "\n",
    "## Reference in blog\n",
    "- [Part 1 Object Detection using YOLOv2 on Pascal VOC2012 - anchor box clustering](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html)\n",
    "- [Part 2 Object Detection using YOLOv2 on Pascal VOC2012 - input and output encoding](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html)\n",
    "- [Part 3 Object Detection using YOLOv2 on Pascal VOC2012 - model](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2012_data_model.html)\n",
    "- [Part 4 Object Detection using YOLOv2 on Pascal VOC2012 - loss](https://fairyonice.github.io/Part_4_Object_Detection_with_Yolo_using_VOC_2012_data_loss.html)\n",
    "- [Part 5 Object Detection using YOLOv2 on Pascal VOC2012 - training](https://fairyonice.github.io/Part_5_Object_Detection_with_Yolo_using_VOC_2012_data_training.html)\n",
    "- [Part 6 Object Detection using YOLOv2 on Pascal VOC 2012 data - inference on image](https://fairyonice.github.io/Part_6_Object_Detection_with_Yolo_using_VOC_2012_data_inference_image.html)\n",
    "- [Part 7 Object Detection using YOLOv2 on Pascal VOC 2012 data - inference on video](https://fairyonice.github.io/Part_7_Object_Detection_with_Yolo_using_VOC_2012_data_inference_video.html)\n",
    "\n",
    "## fairyonice GitHub repository \n",
    "This repository contains all the ipython notebooks in this blog series and the funcitons (See backend.py). \n",
    "- [FairyOnIce/ObjectDetectionYolo](https://github.com/FairyOnIce/ObjectDetectionYolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "print(sys.version)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas: \", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define anchor box\n",
    "<code>ANCHORS</code> defines the number of anchor boxes and the shape of each anchor box.\n",
    "The choice of the anchor box specialization is already discussed in [Part 1 Object Detection using YOLOv2 on Pascal VOC2012 - anchor box clustering](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html). \n",
    "\n",
    "Based on the K-means analysis in the previous blog post, I will select 4 anchor boxes of following width and height. The width and heights are rescaled in the grid cell scale (Assuming that the number of grid size is 13 by 13.) See [Part 2 Object Detection using YOLOv2 on Pascal VOC2012 - input and output encoding](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html) to learn how I rescal the anchor box shapes into the grid cell scale.\n",
    "\n",
    "Here I choose 4 anchor boxes. With 13 by 13 grids, every frame gets 4 x 13 x 13 = 676 bouding box predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHORS = np.array([0.06960639, 0.06130531,\n",
    "                    0.11246752, 0.10739992])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Label vector containing 20 object classe names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['wheat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read images and annotations into memory\n",
    "Use the pre-processing code for parsing annotation at [experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2).\n",
    "This <code>parse_annoation</code> function is already used in [Part 1 Object Detection using YOLOv2 on Pascal VOC2012 - anchor box clustering](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html) and saved in my python script. \n",
    "This script can be downloaded at [my Github repository, FairyOnIce/ObjectDetectionYolo/backend](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Google Collab\n",
    "#ROOT_PATH=\"./\"    ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "\n",
    "# Home\n",
    "ROOT_PATH = \"/Users/john/Documents/Python-Working/Kaggle-global-wheat-detection/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "\n",
    "# Kaggle\n",
    "#ROOT_PATH = \"../input/global-wheat-detection/\"  ###### CHANGE FOR SPECIFIC ENVIRONMENT\n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = os.path.join(ROOT_PATH, \"train/\")\n",
    "TEST_DATA_PATH = os.path.join(ROOT_PATH, \"test/\")\n",
    "MODEL_NAME = \"model-wheat.h5\"\n",
    "\n",
    "BATCH_SIZE        = 32  # 200\n",
    "IMAGE_H, IMAGE_W  = 1024, 1024\n",
    "GRID_H,  GRID_W   = 13 , 13\n",
    "TRUE_BOX_BUFFER   = 1183 #50\n",
    "BOX               = int(len(ANCHORS)/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csvTRAIN_DATA_PATH\n",
    "train_raw_df = pd.read_csv(os.path.join(ROOT_PATH, 'train.csv'))\n",
    "train_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend import parse_annotation\n",
    "np.random.seed(10)\n",
    "train_image = parse_annotation(train_raw_df, LABELS, TRAIN_DATA_PATH, use_dict_for_bboxes=False)\n",
    "print(\"N train = {}\".format(len(train_image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "train_augmentations = albu.Compose([\n",
    "                                   albu.RandomSizedBBoxSafeCrop(IMAGE_H, IMAGE_W, p=1),\n",
    "                                   albu.Flip(p=0.5),\n",
    "                                   albu.OneOf([\n",
    "                                               albu.HueSaturationValue(),\n",
    "                                               albu.RandomBrightnessContrast()\n",
    "                                              ], p=1),\n",
    "                                   albu.GaussNoise(p=0.25),\n",
    "                                   albu.CLAHE(p=1),\n",
    "                                   albu.ToGray(p=1),\n",
    "                                  ], \n",
    "                                  bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "val_augmentations = albu.Compose([\n",
    "                                   albu.CLAHE(p=1),\n",
    "                                   albu.ToGray(p=1),\n",
    "                                  ], \n",
    "                                  bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate batch generator object\n",
    "<code>SimpleBatchGenerator</code> is discussed and used in \n",
    "[Part 2 Object Detection using YOLOv2 on Pascal VOC2012 - input and output encoding](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html).\n",
    "This script can be downloaded at [my Github repository, FairyOnIce/ObjectDetectionYolo/backend](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend import SimpleBatchGenerator, ImageReaderAlbumentations\n",
    "\n",
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'LABELS'          : LABELS,\n",
    "    'ANCHORS'         : ANCHORS,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : TRUE_BOX_BUFFER,\n",
    "}\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    return image / 255.\n",
    "\n",
    "augment = ImageReaderAlbumentations(IMAGE_H, IMAGE_W, train_augmentations, norm=normalize)\n",
    "\n",
    "train_batch_generator = SimpleBatchGenerator(train_image,\n",
    "                                             generator_config,\n",
    "                                             image_reader=augment,\n",
    "                                             shuffle=True)\n",
    "\n",
    "print(len(train_image))\n",
    "[x_batch, y_batch, b_batch] = train_batch_generator.__getitem__(idx=0)\n",
    "print(x_batch.shape, b_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "We define a YOLO model.\n",
    "The model defenition function is already discussed in [Part 3 Object Detection using YOLOv2 on Pascal VOC2012 - model](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2012_data_model.html) and all the codes are available at [my Github](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "We already discussed the loss function of YOLOv2 implemented by [experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2) in [Part 4 Object Detection using YOLOv2 on Pascal VOC2012 - loss](https://fairyonice.github.io/Part_4_Object_Detection_with_Yolo_using_VOC_2012_data_loss.html).\n",
    "I modified the codes and the codes are available at [my Github](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE        = 32  # 32 200\n",
    "IMAGE_H, IMAGE_W  = 416, 416\n",
    "GRID_H,  GRID_W   = 13 , 13\n",
    "TRUE_BOX_BUFFER   = 1183 #50\n",
    "BOX               = int(len(ANCHORS)/2)\n",
    "\n",
    "\n",
    "from backend import custom_loss_core \n",
    "#help(custom_loss_core)\n",
    "\n",
    "LAMBDA_NO_OBJECT = 1.0\n",
    "LAMBDA_OBJECT    = 5.0\n",
    "LAMBDA_COORD     = 1.0\n",
    "LAMBDA_CLASS     = 1.0\n",
    "\n",
    "def custom_loss(y_true, y_pred, true_boxes):\n",
    "    loss = custom_loss_core(y_true,\n",
    "                     y_pred,\n",
    "                     true_boxes,\n",
    "                     GRID_W,\n",
    "                     GRID_H,\n",
    "                     BATCH_SIZE,\n",
    "                     ANCHORS,\n",
    "                     LAMBDA_COORD,\n",
    "                     LAMBDA_CLASS,\n",
    "                     LAMBDA_NO_OBJECT,\n",
    "                     LAMBDA_OBJECT)\n",
    "    #print(\"**** \", loss)\n",
    "    return loss\n",
    "\n",
    "print(custom_loss(y_batch, y_batch, b_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from backend import define_YOLOv2, set_pretrained_weight, initialize_weight\n",
    "\n",
    "CLASS = len(LABELS)\n",
    "(model, y_true, y_pred, true_boxes) = define_YOLOv2(IMAGE_H,\n",
    "                                                    IMAGE_W,\n",
    "                                                    GRID_H,\n",
    "                                                    GRID_W,\n",
    "                                                    TRUE_BOX_BUFFER,\n",
    "                                                    BOX,CLASS, \n",
    "                                                    trainable=True)\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights\n",
    "The initialization of weights are already discussed in [Part 3 Object Detection using YOLOv2 on Pascal VOC2012 - model](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2012_data_model.html). \n",
    "All the codes from [Part 3](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2012_data_model.html) are stored at [my Github](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weight = \"./yolov2.weights\"\n",
    "\n",
    "nb_conv        = 22\n",
    "model          = set_pretrained_weight(model,nb_conv, path_to_weight)\n",
    "layer          = model.layers[-4] # -4 the last convolutional layer\n",
    "initialize_weight(layer,sd=1/(GRID_H*GRID_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.add_loss(custom_loss(y_true, y_pred, true_boxes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this custom function <code>custom_loss_core</code> depends not only on <code>y_true</code> and <code>y_pred</code> but also the various hayperparameters.\n",
    "Unfortunately, Keras's loss function API does not accept any parameters except <code>y_true</code> and <code>y_pred</code>. Therefore, these hyperparameters need to be defined globaly. \n",
    "To do this, I will define a wrapper function <code>custom_loss</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training starts here! \n",
    "Finally, we start the training here.\n",
    "We only train the final 23rd layer and freeze the other weights.\n",
    "This is because I am unfortunately using CPU environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "dir_log = \"logs/\"\n",
    "try:\n",
    "    os.makedirs(dir_log)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "generator_config['BATCH_SIZE'] = BATCH_SIZE\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', \n",
    "                           min_delta=0.001, \n",
    "                           patience=3, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(MODEL_NAME, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "#optimizer = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "#model.compile(loss=custom_loss, optimizer=optimizer)\n",
    "model.compile(loss=None, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_batch_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.fit(train_batch_generator, \n",
    "                    #steps_per_epoch  = len(train_batch_generator),  ##\n",
    "                    epochs           = 10, ##50\n",
    "                    #verbose          = 1,  ##\n",
    "                    #validation_data  = valid_batch,\n",
    "                    #validation_steps = len(valid_batch),\n",
    "                    callbacks        = [early_stop, checkpoint],  ##\n",
    "                    #max_queue_size   = 3 ##\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
